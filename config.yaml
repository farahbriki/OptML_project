SGD:
  learning_rate: 0.01
SGD_Momentum:
  learning_rate: 0.01
  momentum: 0.9
AdaDelta:
  learning_rate: 0.001
  decay: 0.95
Adam:
  learning_rate: 0.001
  betas: (0.9, 0.999)
AdaMax:
  learning_rate: 0.001
  betas: (0.9, 0.999)
NAdam:
  learning_rate: 0.001
  betas: (0.9, 0.999)
AdaGrad:
  learning_rate: 0.001
RMSProp:
  learning_rate: 0.001
  alpha: 0.99